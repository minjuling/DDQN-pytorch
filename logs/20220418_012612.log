[Mon Apr 18 01:26:14 2022]starts experiments setting 4_18_-2_26
=> creating model ...
frames: 100 step: 100 ep_num: 0 Reward: -2.0 Loss: 0.00027257465990260243
frames: 200 step: 200 ep_num: 0 Reward: -3.0 Loss: 0.00040151638677343726
frames: 300 step: 300 ep_num: 0 Reward: -6.0 Loss: 0.0006110745016485453
frames: 400 step: 400 ep_num: 0 Reward: -8.0 Loss: 0.00039907399332150817
frames: 500 step: 500 ep_num: 0 Reward: -11.0 Loss: 0.028696920722723007
frames: 600 step: 600 ep_num: 0 Reward: -14.0 Loss: 0.03140892833471298
frames: 700 step: 700 ep_num: 0 Reward: -17.0 Loss: 0.06084367632865906
frames: 800 step: 800 ep_num: 0 Reward: -20.0 Loss: 0.0008286347147077322
episode: 823 Reward per episode: -21.0  
Did not solve after 1 episodes
frames: 900 step: 900 ep_num: 1 Reward: -1.0 Loss: 0.0006154232542030513
frames: 1000 step: 1000 ep_num: 1 Reward: -4.0 Loss: 0.06009100005030632
frames: 1100 step: 1100 ep_num: 1 Reward: -7.0 Loss: 0.0005643533659167588
frames: 1200 step: 1200 ep_num: 1 Reward: -10.0 Loss: 0.000956608506385237
frames: 1300 step: 1300 ep_num: 1 Reward: -12.0 Loss: 0.0007279217825271189
frames: 1400 step: 1400 ep_num: 1 Reward: -15.0 Loss: 0.00047721571172587574
frames: 1500 step: 1500 ep_num: 1 Reward: -18.0 Loss: 0.030879005789756775
episode: 1584 Reward per episode: -21.0  
Did not solve after 2 episodes
frames: 1600 step: 1600 ep_num: 2 Reward: 0.0 Loss: 0.0005594100803136826
frames: 1700 step: 1700 ep_num: 2 Reward: -2.0 Loss: 0.0006031572120264173
frames: 1800 step: 1800 ep_num: 2 Reward: -4.0 Loss: 0.026859339326620102
frames: 1900 step: 1900 ep_num: 2 Reward: -7.0 Loss: 0.0007571411551907659
frames: 2000 step: 2000 ep_num: 2 Reward: -10.0 Loss: 0.0014956723898649216
frames: 2100 step: 2100 ep_num: 2 Reward: -13.0 Loss: 0.00021200336050242186
frames: 2200 step: 2200 ep_num: 2 Reward: -15.0 Loss: 0.00015322411491069943
frames: 2300 step: 2300 ep_num: 2 Reward: -18.0 Loss: 0.00010682539868867025
frames: 2400 step: 2400 ep_num: 2 Reward: -21.0 Loss: 0.03011512942612171
episode: 2400 Reward per episode: -21.0  
Did not solve after 3 episodes
frames: 2500 step: 2500 ep_num: 3 Reward: -2.0 Loss: 0.00012117448204662651
frames: 2600 step: 2600 ep_num: 3 Reward: -5.0 Loss: 0.0008109395857900381
frames: 2700 step: 2700 ep_num: 3 Reward: -8.0 Loss: 0.00036151634412817657
frames: 2800 step: 2800 ep_num: 3 Reward: -10.0 Loss: 0.00033959950087592006
frames: 2900 step: 2900 ep_num: 3 Reward: -13.0 Loss: 9.932978719007224e-05
frames: 3000 step: 3000 ep_num: 3 Reward: -16.0 Loss: 0.0007020768825896084
frames: 3100 step: 3100 ep_num: 3 Reward: -18.0 Loss: 0.0003027574566658586
frames: 3200 step: 3200 ep_num: 3 Reward: -20.0 Loss: 0.00025957735488191247
episode: 3215 Reward per episode: -21.0  
Did not solve after 4 episodes
frames: 3300 step: 3300 ep_num: 4 Reward: 0.0 Loss: 0.0003818462137132883
frames: 3400 step: 3400 ep_num: 4 Reward: -1.0 Loss: 0.03125328570604324
frames: 3500 step: 3500 ep_num: 4 Reward: -2.0 Loss: 0.00011053610069211572
frames: 3600 step: 3600 ep_num: 4 Reward: -4.0 Loss: 0.0001081248774426058
frames: 3700 step: 3700 ep_num: 4 Reward: -7.0 Loss: 0.006854063831269741
frames: 3800 step: 3800 ep_num: 4 Reward: -9.0 Loss: 0.0003076959401369095
frames: 3900 step: 3900 ep_num: 4 Reward: -12.0 Loss: 0.00025153529713861644
frames: 4000 step: 4000 ep_num: 4 Reward: -15.0 Loss: 0.0003645287360996008
frames: 4100 step: 4100 ep_num: 4 Reward: -18.0 Loss: 0.00033136631827801466
episode: 4191 Reward per episode: -20.0  
Did not solve after 5 episodes
frames: 4200 step: 4200 ep_num: 5 Reward: 0.0 Loss: 0.00015028291090857238
frames: 4300 step: 4300 ep_num: 5 Reward: -2.0 Loss: 0.0002395085321040824
frames: 4400 step: 4400 ep_num: 5 Reward: -5.0 Loss: 0.000627920322585851
frames: 4500 step: 4500 ep_num: 5 Reward: -7.0 Loss: 4.1082632378675044e-05
frames: 4600 step: 4600 ep_num: 5 Reward: -10.0 Loss: 0.00019349381909705698
frames: 4700 step: 4700 ep_num: 5 Reward: -12.0 Loss: 3.9823436964070424e-05
